{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Predicting\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forked From Kaggle - Andy Harless - XGBoost, LightGBM, OLS and NN\n",
    "Public Score\n",
    "0.0643646"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Global Hyper Parameters\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FUDGE_FACTOR = 1.1200  # Multiply forecasts by this\n",
    "\n",
    "XGB_WEIGHT = 0.6200\n",
    "BASELINE_WEIGHT = 0.0100\n",
    "OLS_WEIGHT = 0.0620\n",
    "NN_WEIGHT = 0.0800\n",
    "\n",
    "XGB1_WEIGHT = 0.8000  # Weight of first in combination of two XGB models\n",
    "\n",
    "BASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import Packages\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 5110 on context None\n",
      "Preallocating 9011/11264 Mb (0.800000) on cuda0\n",
      "Mapped name None to device cuda0: GeForce GTX 1080 Ti (0000:06:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler,Normalizer\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error,make_scorer\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "import random\n",
    "import datetime as dt\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Preparation\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_only=True\n",
    "clip_norm=(-0.4,0.419)\n",
    "month_train=range(1,7)\n",
    "month_test=range(7,10)\n",
    "category_features=['propertyzoningdesc','taxdelinquencyyear','propertycountylandusecode','airconditioningtypeid', 'architecturalstyletypeid', 'buildingclasstypeid', 'buildingqualitytypeid', 'decktypeid', 'fips', 'hashottuborspa', 'heatingorsystemtypeid', 'poolcnt', 'pooltypeid10', 'pooltypeid2', 'pooltypeid7', 'propertylandusetypeid', 'regionidcounty', 'storytypeid', 'threequarterbathnbr', 'typeconstructiontypeid', 'numberofstories', 'fireplaceflag', 'taxdelinquencyflag']+['basementsqft', 'finishedfloor1squarefeet', 'finishedsquarefeet13', 'finishedsquarefeet15', 'finishedsquarefeet50', 'finishedsquarefeet6', 'fireplacecnt', 'garagecarcnt', 'garagetotalsqft', 'poolsizesum', 'yardbuildingsqft17', 'yardbuildingsqft26']\n",
    "numerical_features=[ 'yearbuilt', 'assessmentyear','bathroomcnt', 'bedroomcnt', 'calculatedbathnbr', 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'fullbathcnt', 'lotsizesquarefeet', 'roomcnt', 'unitcnt', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount']\n",
    "extra_features=['latitude', 'longitude',   'rawcensustractandblock', 'regionidcity', 'regionidzip',  'censustractandblock']\n",
    "prediction_columns=['201607','201608','201609','201707','201708','201709']\n",
    "prop_lightgbm_path='Data/prop_clean.csv'\n",
    "prop_xgboost_path='Data/prop_clean.csv'\n",
    "prop_ols_path='Data/prop_clean.csv'\n",
    "prop_nn_path='Data/prop_clean.csv'\n",
    "train_path='Data/train.csv'\n",
    "sample_path='Data/bak/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loader(properties_path,drop_list,outlier_bound=(None,None),parse_date=False,labelencode=False,onehotencode=False):\n",
    "    print( \"\\nReading data from disk ...\")\n",
    "    print(\"   Read properties file ...\")\n",
    "    properties = pd.read_csv(properties_path)\n",
    "    print(\"   ...\")\n",
    "    if onehotencode:\n",
    "        print(\"   One Hot Encoding ...\")\n",
    "        properties=pd.get_dummies(properties, columns=category_features, drop_first=False)\n",
    "        print(\"   ...\")\n",
    "    if labelencode:\n",
    "        print(\"   Label Encoding ...\")\n",
    "        for c in category_features:\n",
    "            le=LabelEncoder()\n",
    "            le.fit(properties[c])\n",
    "            properties[c]=le.transform(properties[c])\n",
    "        print(\"   ...\")\n",
    "    print(\"   Read training file ...\")\n",
    "    train = pd.read_csv(train_path, parse_dates=[\"transactiondate\"])\n",
    "    print(\"   ...\")\n",
    "    train_properties = train.merge(properties, how='left', on='parcelid')\n",
    "    if parse_date:\n",
    "        train_properties[\"transactiondate_year\"] = train_properties[\"transactiondate\"].dt.year\n",
    "        train_properties[\"transactiondate_month\"] = train_properties[\"transactiondate\"].dt.month\n",
    "    if outlier_bound[0] is not None:\n",
    "        train_properties=train_properties[train_properties.logerror > outlier_bound[0]]\n",
    "    if outlier_bound[1] is not None:\n",
    "        train_properties=train_properties[train_properties.logerror < outlier_bound[1]]\n",
    "    train_properties=train_properties[train_properties['transactiondate'].dt.month.isin(month_train)]\n",
    "    train_drop_list=copy.copy(drop_list)\n",
    "    train_drop_list_all=copy.copy(train_drop_list)\n",
    "    for dropitem in train_drop_list_all:\n",
    "        if dropitem not in train_properties.columns.tolist():\n",
    "            train_drop_list.remove(dropitem)\n",
    "    y_train = train_properties['logerror']\n",
    "    x_train = train_properties.drop(train_drop_list, axis=1)\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    print(\"\\nPrepare for prediction ...\")\n",
    "    print(\"   Read sample file ...\")\n",
    "    sample = pd.read_csv(sample_path)\n",
    "    sample['parcelid'] = sample['ParcelId']\n",
    "    print(\"   ...\")\n",
    "    print(\"   Merge with property data ...\")\n",
    "    if test_only:\n",
    "        test_properties = train.merge(properties, how='left', on='parcelid')\n",
    "        test_properties=test_properties[test_properties['transactiondate'].dt.month.isin(month_test)]\n",
    "    else:\n",
    "        test_properties = sample.merge(properties, on='parcelid', how='left')\n",
    "    parcelid=test_properties['parcelid']\n",
    "    if parse_date:\n",
    "        test_properties[\"transactiondate_year\"] = 2016.5\n",
    "        test_properties[\"transactiondate_month\"] = np.mean(month_test)\n",
    "    test_drop_list=copy.copy(drop_list)+['ParcelId','201610','201611','201612','201710','201711','201712']\n",
    "    test_drop_list_all=copy.copy(test_drop_list)\n",
    "    for dropitem in test_drop_list_all:\n",
    "        if dropitem not in test_properties.columns.tolist():\n",
    "            test_drop_list.remove(dropitem)\n",
    "    x_test=test_properties.drop(test_drop_list, axis=1)\n",
    "    print(x_test.shape)\n",
    "    return x_train,y_train,x_test,parcelid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x_test):\n",
    "    ssr=StandardScaler()\n",
    "    ssr.fit(x_test)\n",
    "    return ssr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid = pd.read_csv(train_path, parse_dates=[\"transactiondate\"])\n",
    "y_true=OrderedDict()\n",
    "y_mask=[]\n",
    "y_true = valid.logerror[valid['transactiondate'].dt.month.isin(month_test)]\n",
    "y_date = valid.transactiondate[valid['transactiondate'].dt.month.isin(month_test)]\n",
    "for yr in [2016,2017]:\n",
    "    for mt in month_test:\n",
    "        y_mask.append(np.bitwise_and(y_date.dt.month==mt,y_date.dt.year==yr))\n",
    "y_mask=np.asarray(y_mask)\n",
    "y_mask=y_mask.T.astype('int64')\n",
    "def validate(y_test):\n",
    "    return np.mean(np.abs(np.sum(y_test*y_mask,1)-y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. LightGBM\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for LightGBM ...\n",
      "\n",
      "Reading data from disk ...\n",
      "   Read properties file ...\n",
      "   ...\n",
      "   Label Encoding ...\n",
      "   ...\n",
      "   Read training file ...\n",
      "   ...\n",
      "((105121, 57), (105121L,))\n",
      "\n",
      "Prepare for prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "(54225, 57)\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nProcessing data for LightGBM ...\" )\n",
    "\n",
    "x_train,y_train,x_test,parcelid=loader(prop_lightgbm_path,['parcelid', 'logerror', 'transactiondate'],labelencode=True)\n",
    "features=category_features+numerical_features+extra_features\n",
    "d_train = lgb.Dataset(x_train[features], label=y_train, categorical_feature=category_features,free_raw_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set LightGBM Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['max_bin'] = 10\n",
    "params['learning_rate'] = 0.0021 # shrinkage_rate\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'l1'          # or 'mae'\n",
    "params['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\n",
    "params['bagging_fraction'] = 0.85 # sub_row\n",
    "params['bagging_freq'] = 40\n",
    "params['num_leaves'] = 512        # num_leaf\n",
    "params['min_data'] = 500         # min_data_in_leaf\n",
    "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n",
    "params['verbose'] = 0\n",
    "params['feature_fraction_seed'] = 2\n",
    "params['bagging_seed'] = 3\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting LightGBM model ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFitting LightGBM model ...\")\n",
    "\n",
    "clf = lgb.train(params, d_train, 430, categorical_feature=category_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start LightGBM prediction ...\n",
      "\n",
      "Unadjusted LightGBM predictions:\n",
      "          0\n",
      "0  0.021860\n",
      "1  0.022435\n",
      "2  0.022740\n",
      "3  0.021015\n",
      "4  0.022069\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStart LightGBM prediction ...\")\n",
    "\n",
    "lgb_pred = clf.predict(x_test)\n",
    "\n",
    "print( \"\\nUnadjusted LightGBM predictions:\" )\n",
    "print( pd.DataFrame(lgb_pred).head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54225L,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate For LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06942165970198744"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(np.tile(lgb_pred[:,None],[1,6]),clip_norm[0],clip_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train,y_train,x_test,parcelid,d_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. XGBoost\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data For XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for XGBoost ...\n",
      "\n",
      "Reading data from disk ...\n",
      "   Read properties file ...\n",
      "   ...\n",
      "   Label Encoding ...\n",
      "   ...\n",
      "   Read training file ...\n",
      "   ...\n",
      "((102914, 57), (102914L,))\n",
      "\n",
      "Prepare for prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "(54225, 57)\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nProcessing data for XGBoost ...\")\n",
    "\n",
    "x_train,y_train,x_test,parcelid=loader(prop_lightgbm_path,['parcelid', 'logerror', 'transactiondate'],outlier_bound=clip_norm,labelencode=True)\n",
    "features=category_features+numerical_features+extra_features\n",
    "y_mean = np.mean(y_train)\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set XGBoost Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "num_boost_rounds = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting XGBoost model ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFitting XGBoost model ...\")\n",
    "\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with XGBoost ...\n",
      "\n",
      "First XGBoost predictions:\n",
      "          0\n",
      "0  0.003354\n",
      "1  0.008021\n",
      "2 -0.002644\n",
      "3  0.009713\n",
      "4 -0.001039\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nPredicting with XGBoost ...\")\n",
    "\n",
    "xgb_pred1 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nFirst XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred1).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate For First XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06695654820228315"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(np.tile(xgb_pred1[:,None],[1,6]),clip_norm[0],clip_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set XGBoost Hyper Parameters Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'eta': 0.033,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "num_boost_rounds = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run XGBoost Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost again ...\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nTraining XGBoost again ...\")\n",
    "\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Prediction Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with XGBoost again ...\n",
      "\n",
      "Second XGBoost predictions:\n",
      "          0\n",
      "0  0.003513\n",
      "1  0.008814\n",
      "2 -0.000905\n",
      "3  0.009317\n",
      "4  0.000328\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nPredicting with XGBoost again ...\")\n",
    "\n",
    "xgb_pred2 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nSecond XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred2).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate For Second XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0669726678132372"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(np.tile(xgb_pred2[:,None],[1,6]),clip_norm[0],clip_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Twice XGBoost Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined XGBoost predictions:\n",
      "          0\n",
      "0  0.003386\n",
      "1  0.008179\n",
      "2 -0.002297\n",
      "3  0.009634\n",
      "4 -0.000765\n"
     ]
    }
   ],
   "source": [
    "xgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n",
    "\n",
    "print( \"\\nCombined XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate For Combined XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06694858812380969"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(np.tile(xgb_pred[:,None],[1,6]),clip_norm[0],clip_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train,y_train,x_test,parcelid,dtrain,dtest\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Neural Network\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data For Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing data for Neural Network ...\n",
      "\n",
      "Reading data from disk ...\n",
      "   Read properties file ...\n",
      "   ...\n",
      "   One Hot Encoding ...\n",
      "   ...\n",
      "   Read training file ...\n",
      "   ...\n",
      "((105121, 177), (105121L,))\n",
      "\n",
      "Prepare for prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "(54225, 177)\n",
      "(105121L, 177L)\n"
     ]
    }
   ],
   "source": [
    "print( \"\\n\\nProcessing data for Neural Network ...\")\n",
    "\n",
    "x_train,y_train,x_test,parcelid=loader(prop_lightgbm_path,['parcelid', 'logerror','transactiondate']+extra_features,parse_date=True,onehotencode=True)\n",
    "ssr=normalize(x_test)\n",
    "x_train=ssr.transform(x_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "len_x=x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up neural network model...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSetting up neural network model...\")\n",
    "nn = Sequential()\n",
    "nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = len_x))\n",
    "nn.add(PReLU())\n",
    "nn.add(Dropout(.4))\n",
    "nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.5))\n",
    "nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(1, kernel_initializer='normal'))\n",
    "nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting neural network model...\n",
      "0.0699696842884\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFitting neural network model...\")\n",
    "\n",
    "nnhistory=nn.fit(np.array(x_train), np.array(y_train), batch_size = 4096, epochs = 70, verbose=0)\n",
    "\n",
    "print(nnhistory.history['loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with neural network model...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "\n",
      "Neural Network predictions:\n",
      "      parcelid    201607    201608    201609    201707    201708    201709\n",
      "782   14677191  0.004404  0.004402  0.004400  0.004405  0.004403  0.004401\n",
      "968   11183209  0.004399  0.004403  0.004407  0.004400  0.004403  0.004407\n",
      "1165  11554091  0.004373  0.004375  0.004378  0.004376  0.004377  0.004379\n",
      "1351  11742566  0.004386  0.004388  0.004393  0.004387  0.004389  0.004394\n",
      "1609  14667297  0.004405  0.004404  0.004403  0.004406  0.004405  0.004404\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredicting with neural network model...\")\n",
    "\n",
    "preds_dictionary=OrderedDict()\n",
    "preds_dictionary['parcelid']=parcelid\n",
    "for i,date in enumerate(prediction_columns):\n",
    "    x_test_i=x_test.copy()\n",
    "    x_test_i[\"transactiondate_year\"] = date[:4]\n",
    "    x_test_i[\"transactiondate_month\"] = date[4:]\n",
    "    x_test_i=ssr.transform(x_test_i)\n",
    "    preds_dictionary[date]=nn.predict(x_test_i).flatten()\n",
    "    print \"   ...\"\n",
    "    del x_test_i\n",
    "nn_pred = pd.DataFrame(preds_dictionary)\n",
    "\n",
    "print( \"\\nNeural Network predictions:\" )\n",
    "print( pd.DataFrame(nn_pred).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate For NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06760387642306702"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(nn_pred.values[:,1:],clip_norm[0],clip_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4583"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train,y_train,x_test,parcelid,preds_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. OLS\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data For OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing data for OLS ...\n",
      "\n",
      "Reading data from disk ...\n",
      "   Read properties file ...\n",
      "   ...\n",
      "   One Hot Encoding ...\n",
      "   ...\n",
      "   Read training file ...\n",
      "   ...\n",
      "((105121, 177), (105121L,))\n",
      "\n",
      "Prepare for prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "(54225, 177)\n",
      "(105121L, 177L)\n"
     ]
    }
   ],
   "source": [
    "print( \"\\n\\nProcessing data for OLS ...\")\n",
    "x_train,y_train,x_test,parcelid=loader(prop_lightgbm_path,['parcelid', 'logerror','transactiondate']+extra_features,parse_date=True,onehotencode=True)\n",
    "ssr=normalize(x_test)\n",
    "x_train=ssr.transform(x_train)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting OLS...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nFitting OLS...\")\n",
    "\n",
    "reg = LinearRegression(n_jobs=-1)\n",
    "reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with OLS model...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "\n",
      "OLS predictions:\n",
      "      parcelid    201607    201608    201609    201707    201708    201709\n",
      "782   14677191 -0.006553 -0.008514 -0.010476 -0.002206 -0.004168 -0.006130\n",
      "968   11183209  0.013480  0.011519  0.009557  0.017826  0.015865  0.013903\n",
      "1165  11554091 -0.005324 -0.007286 -0.009248 -0.000978 -0.002940 -0.004901\n",
      "1351  11742566  0.023170  0.021208  0.019246  0.027516  0.025554  0.023593\n",
      "1609  14667297 -0.017631 -0.019592 -0.021554 -0.013284 -0.015246 -0.017208\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredicting with OLS model...\")\n",
    "\n",
    "preds_dictionary=OrderedDict()\n",
    "preds_dictionary['parcelid']=parcelid\n",
    "for i,date in enumerate(prediction_columns):\n",
    "    x_test_i=x_test.copy()\n",
    "    x_test_i[\"transactiondate_year\"] = date[:4]\n",
    "    x_test_i[\"transactiondate_month\"] = date[4:]\n",
    "    x_test_i=ssr.transform(x_test_i)\n",
    "    preds_dictionary[date]=reg.predict(x_test_i)\n",
    "    print \"   ...\"\n",
    "    del x_test_i\n",
    "ols_pred = pd.DataFrame(preds_dictionary)\n",
    "\n",
    "print( \"\\nOLS predictions:\" )\n",
    "print( pd.DataFrame(ols_pred).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate For OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0680084796120211"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(ols_pred.values[:,1:],clip_norm[0],clip_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train,y_train,x_test,preds_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Combine And Save\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Predictionsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining XGBoost, LightGBM, and baseline predicitons ...\n",
      "\n",
      "Combined XGB/LGB/baseline predictions:\n",
      "          0\n",
      "0  0.007198\n",
      "1  0.010301\n",
      "2  0.003876\n",
      "3  0.010879\n",
      "4  0.004672\n",
      "\n",
      "Combining with XGB/LGB/NN/OLS/baseline predicitons: ...\n",
      "\n",
      "Combined XGB/LGB/NN/baseline/OLS predictions:\n",
      "      parcelid  201607  201608  201609  201707  201708  201709\n",
      "782   14677191  0.0080  0.0079  0.0077  0.0083  0.0082  0.0080\n",
      "968   11183209  0.0129  0.0127  0.0126  0.0132  0.0130  0.0129\n",
      "1165  11554091  0.0044  0.0042  0.0041  0.0047  0.0045  0.0044\n",
      "1351  11742566  0.0142  0.0141  0.0139  0.0145  0.0144  0.0142\n",
      "1609  14667297  0.0044  0.0043  0.0041  0.0047  0.0046  0.0044\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nCombining XGBoost, LightGBM, and baseline predicitons ...\" )\n",
    "lgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - NN_WEIGHT - OLS_WEIGHT \n",
    "pred0 = BASELINE_WEIGHT*BASELINE_PRED\n",
    "pred0 += lgb_weight*lgb_pred\n",
    "pred0 += XGB_WEIGHT*xgb_pred\n",
    "\n",
    "print( \"\\nCombined XGB/LGB/baseline predictions:\" )\n",
    "print( pd.DataFrame(pred0).head() )\n",
    "\n",
    "print( \"\\nCombining with XGB/LGB/NN/OLS/baseline predicitons: ...\" )\n",
    "pred0 =np.tile(pred0[:,None],[1,6]) + NN_WEIGHT*nn_pred.values[:,1:]\n",
    "pred0 = FUDGE_FACTOR * ( OLS_WEIGHT*ols_pred.values[:,1:] + pred0 )\n",
    "submission_dict=OrderedDict()\n",
    "submission_dict['parcelid']=parcelid\n",
    "for i,date in enumerate(prediction_columns):\n",
    "    submission_dict[date]=pred0[:,i]\n",
    "submission=pd.DataFrame(submission_dict)\n",
    "for c in submission.columns[1:]:\n",
    "    submission[c]=submission[c].apply(lambda x:float(format(x, '.4f')))\n",
    "\n",
    "print( \"\\nCombined XGB/LGB/NN/baseline/OLS predictions:\" )\n",
    "print( submission.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06703410937545362"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(submission.values[:,1:],clip_norm[0],clip_norm[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

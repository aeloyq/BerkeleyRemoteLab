{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Predicting\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forked From Kaggle - Andy Harless - XGBoost, LightGBM, OLS and NN\n",
    "Public Score\n",
    "0.0643646"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Global Hyper Parameters\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FUDGE_FACTOR = 1.1200  # Multiply forecasts by this\n",
    "\n",
    "XGB_WEIGHT = 0.6200\n",
    "BASELINE_WEIGHT = 0.0100\n",
    "OLS_WEIGHT = 0.0620\n",
    "NN_WEIGHT = 0.0800\n",
    "\n",
    "XGB1_WEIGHT = 0.8000  # Weight of first in combination of two XGB models\n",
    "\n",
    "BASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import Packages\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 5110 on context None\n",
      "Preallocating 9011/11264 Mb (0.800000) on cuda0\n",
      "Mapped name None to device cuda0: GeForce GTX 1080 Ti (0000:06:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler,Normalizer\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error,make_scorer\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "import random\n",
    "import datetime as dt\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Preparation\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "month_train=range(1,13)\n",
    "month_test=range(10,13)\n",
    "category_features=['propertyzoningdesc','taxdelinquencyyear','propertycountylandusecode','airconditioningtypeid', 'architecturalstyletypeid', 'buildingclasstypeid', 'buildingqualitytypeid', 'decktypeid', 'fips', 'hashottuborspa', 'heatingorsystemtypeid', 'poolcnt', 'pooltypeid10', 'pooltypeid2', 'pooltypeid7', 'propertylandusetypeid', 'regionidcounty', 'storytypeid', 'threequarterbathnbr', 'typeconstructiontypeid', 'numberofstories', 'fireplaceflag', 'taxdelinquencyflag']+['basementsqft', 'finishedfloor1squarefeet', 'finishedsquarefeet13', 'finishedsquarefeet15', 'finishedsquarefeet50', 'finishedsquarefeet6', 'fireplacecnt', 'garagecarcnt', 'garagetotalsqft', 'poolsizesum', 'yardbuildingsqft17', 'yardbuildingsqft26']\n",
    "numerical_features=[ 'yearbuilt', 'assessmentyear','bathroomcnt', 'bedroomcnt', 'calculatedbathnbr', 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'fullbathcnt', 'lotsizesquarefeet', 'roomcnt', 'unitcnt', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount']\n",
    "extra_features=['latitude', 'longitude',   'rawcensustractandblock', 'regionidcity', 'regionidzip',  'censustractandblock']\n",
    "prediction_dates=['2016-10-15','2016-11-15','2016-12-15','2017-10-15','2017-11-15','2017-12-15']\n",
    "prediction_columns=['201610','201611','201612','201710','201711','201712']\n",
    "prop_lightgbm_path='Data/prop_clean.csv'\n",
    "prop_xgboost_path='Data/prop_clean.csv'\n",
    "prop_ols_path='Data/prop_clean.csv'\n",
    "prop_nn_path='Data/prop_clean.csv'\n",
    "train_path='Data/train.csv'\n",
    "sample_path='Data/bak/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loader(properties_path,drop_list,outlier_bound=(None,None),parse_date=False,labelencode=False,onehotencode=False,test_only=False):\n",
    "    print( \"\\nReading data from disk ...\")\n",
    "    print(\"   Read properties file ...\")\n",
    "    properties = pd.read_csv(properties_path)\n",
    "    print(\"   ...\")\n",
    "    if onehotencode:\n",
    "        print(\"   One Hot Encoding ...\")\n",
    "        properties=pd.get_dummies(properties, columns=category_features, drop_first=False)\n",
    "        print(\"   ...\")\n",
    "    if labelencode:\n",
    "        print(\"   Label Encoding ...\")\n",
    "        for c in category_features:\n",
    "            le=LabelEncoder()\n",
    "            le.fit(properties[c])\n",
    "            properties[c]=le.transform(properties[c])\n",
    "        print(\"   ...\")\n",
    "    print(\"   Read training file ...\")\n",
    "    train = pd.read_csv(train_path, parse_dates=[\"transactiondate\"])\n",
    "    print(\"   ...\")\n",
    "    train_properties = train.merge(properties, how='left', on='parcelid')\n",
    "    if parse_date:\n",
    "        train_properties[\"transactiondate_year\"] = train_properties[\"transactiondate\"].dt.year\n",
    "        train_properties[\"transactiondate_month\"] = train_properties[\"transactiondate\"].dt.month\n",
    "    if outlier_bound[0] is not None:\n",
    "        train_properties=train_properties[train_properties.logerror > outlier_bound[0]]\n",
    "    if outlier_bound[1] is not None:\n",
    "        train_properties=train_properties[train_properties.logerror < outlier_bound[1]]\n",
    "    train_properties=train_properties[train_properties['transactiondate'].dt.month.isin(month_train)]\n",
    "    train_drop_list=copy.copy(drop_list)\n",
    "    train_drop_list_all=copy.copy(train_drop_list)\n",
    "    for dropitem in train_drop_list_all:\n",
    "        if dropitem not in train_properties.columns.tolist():\n",
    "            train_drop_list.remove(dropitem)\n",
    "    y_train = train_properties['logerror']\n",
    "    x_train = train_properties.drop(train_drop_list, axis=1)\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    print(\"\\nPrepare for prediction ...\")\n",
    "    print(\"   Read sample file ...\")\n",
    "    sample = pd.read_csv(sample_path)\n",
    "    sample['parcelid'] = sample['ParcelId']\n",
    "    print(\"   ...\")\n",
    "    print(\"   Merge with property data ...\")\n",
    "    if test_only:\n",
    "        test_properties = train.merge(properties, how='left', on='parcelid')\n",
    "        test_properties=test_properties[test_properties['transactiondate'].dt.month.isin(month_test)]\n",
    "    else:\n",
    "        test_properties = sample.merge(properties, on='parcelid', how='left')\n",
    "    parcelid=test_properties['parcelid']\n",
    "    if parse_date:\n",
    "        test_properties[\"transactiondate_year\"] = 2016.5\n",
    "        test_properties[\"transactiondate_month\"] = np.mean(month_test)\n",
    "    test_drop_list=copy.copy(drop_list)+['ParcelId','201610','201611','201612','201710','201711','201712']\n",
    "    test_drop_list_all=copy.copy(test_drop_list)\n",
    "    for dropitem in test_drop_list_all:\n",
    "        if dropitem not in test_properties.columns.tolist():\n",
    "            test_drop_list.remove(dropitem)\n",
    "    x_test=test_properties.drop(test_drop_list, axis=1)\n",
    "    print(x_test.shape)\n",
    "    return x_train,y_train,x_test,parcelid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x_test):\n",
    "    ssr=StandardScaler()\n",
    "    ssr.fit(x_test)\n",
    "    return ssr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. LightGBM\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for LightGBM ...\n",
      "\n",
      "Reading data from disk ...\n",
      "   Read properties file ...\n",
      "   ...\n",
      "   Label Encoding ...\n",
      "   ...\n",
      "   Read training file ...\n",
      "   ...\n",
      "((167888, 57), (167888L,))\n",
      "\n",
      "Prepare for prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "(2985217, 57)\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nProcessing data for LightGBM ...\" )\n",
    "\n",
    "x_train,y_train,x_test,parcelid=loader(prop_lightgbm_path,['parcelid', 'logerror', 'transactiondate'],labelencode=True)\n",
    "features=category_features+numerical_features+extra_features\n",
    "d_train = lgb.Dataset(x_train[features], label=y_train, categorical_feature=category_features,free_raw_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set LightGBM Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['max_bin'] = 10\n",
    "params['learning_rate'] = 0.0021 # shrinkage_rate\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'l1'          # or 'mae'\n",
    "params['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\n",
    "params['bagging_fraction'] = 0.85 # sub_row\n",
    "params['bagging_freq'] = 40\n",
    "params['num_leaves'] = 512        # num_leaf\n",
    "params['min_data'] = 500         # min_data_in_leaf\n",
    "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n",
    "params['verbose'] = 0\n",
    "params['feature_fraction_seed'] = 2\n",
    "params['bagging_seed'] = 3\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation For LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross validing LightGBM model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aeloyq\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validated Mean Absolute Error :     0.0623537715374\n",
      "Cross Validated Mean Absolute Error Std :     0.00883101016585\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCross validing LightGBM model ...\")\n",
    "\n",
    "score=lgb.cv(params, d_train, 430, metrics='mae', categorical_feature=category_features)\n",
    "\n",
    "print('Cross Validated Mean Absolute Error :     %s'%(str(score.values()[0][-1])))\n",
    "print('Cross Validated Mean Absolute Error Std :     %s'%(np.mean(score.values()[1][-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting LightGBM model ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFitting LightGBM model ...\")\n",
    "\n",
    "clf = lgb.train(params, d_train, 430, categorical_feature=category_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start LightGBM prediction ...\n",
      "\n",
      "Unadjusted LightGBM predictions:\n",
      "          0\n",
      "0  0.027944\n",
      "1  0.029488\n",
      "2  0.029634\n",
      "3  0.028672\n",
      "4  0.029576\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStart LightGBM prediction ...\")\n",
    "\n",
    "lgb_pred = clf.predict(x_test)\n",
    "\n",
    "print( \"\\nUnadjusted LightGBM predictions:\" )\n",
    "print( pd.DataFrame(lgb_pred).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train,y_train,x_test,parcelid,d_train,score\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. XGBoost\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data For XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for XGBoost ...\n",
      "\n",
      "Reading data from disk ...\n",
      "   Read properties file ...\n",
      "   ...\n",
      "   Label Encoding ...\n",
      "   ...\n",
      "   Read training file ...\n",
      "   ...\n",
      "((164477, 57), (164477L,))\n",
      "\n",
      "Prepare for prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "(2985217, 57)\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nProcessing data for XGBoost ...\")\n",
    "\n",
    "x_train,y_train,x_test,parcelid=loader(prop_lightgbm_path,['parcelid', 'logerror', 'transactiondate'],outlier_bound=(-0.4,0.419),labelencode=True)\n",
    "features=category_features+numerical_features+extra_features\n",
    "y_mean = np.mean(y_train)\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set XGBoost Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "num_boost_rounds = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation For XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross validing XGBoost model ...\n",
      "Cross Validated Mean Absolute Error :     0.0525704\n",
      "Cross Validated Mean Absolute Error Std :     0.000451096708035\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCross validing XGBoost model ...\")\n",
    "\n",
    "score=xgb.cv(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds, nfold=5, metrics='mae')\n",
    "\n",
    "print('Cross Validated Mean Absolute Error :     %s'%(str(score.iloc[-1,0])))\n",
    "print('Cross Validated Mean Absolute Error Std :     %s'%(str(score.iloc[-1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting XGBoost model ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFitting XGBoost model ...\")\n",
    "\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with XGBoost ...\n",
      "\n",
      "First XGBoost predictions:\n",
      "          0\n",
      "0  0.013468\n",
      "1 -0.006351\n",
      "2 -0.045269\n",
      "3 -0.009459\n",
      "4 -0.046619\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nPredicting with XGBoost ...\")\n",
    "\n",
    "xgb_pred1 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nFirst XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred1).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set XGBoost Hyper Parameters Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'eta': 0.033,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "num_boost_rounds = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation For XGBoost  Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross validing LightGBM model ...\n",
      "Cross Validated Mean Absolute Error :     0.0526002\n",
      "Cross Validated Mean Absolute Error Std :     0.000467388660539\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCross validing LightGBM model ...\")\n",
    "\n",
    "score=xgb.cv(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds, nfold=5, metrics='mae')\n",
    "\n",
    "print('Cross Validated Mean Absolute Error :     %s'%(str(score.iloc[-1,0])))\n",
    "print('Cross Validated Mean Absolute Error Std :     %s'%(str(score.iloc[-1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run XGBoost Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost again ...\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nTraining XGBoost again ...\")\n",
    "\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Prediction Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with XGBoost again ...\n",
      "\n",
      "Second XGBoost predictions:\n",
      "          0\n",
      "0  0.029393\n",
      "1 -0.003237\n",
      "2 -0.051331\n",
      "3 -0.001733\n",
      "4 -0.039052\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nPredicting with XGBoost again ...\")\n",
    "\n",
    "xgb_pred2 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nSecond XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred2).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Twice XGBoost Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined XGBoost predictions:\n",
      "          0\n",
      "0  0.016653\n",
      "1 -0.005728\n",
      "2 -0.046481\n",
      "3 -0.007914\n",
      "4 -0.045106\n"
     ]
    }
   ],
   "source": [
    "xgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n",
    "\n",
    "print( \"\\nCombined XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train,y_train,x_test,parcelid,dtrain,dtest\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Neural Network\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data For Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing data for Neural Network ...\n",
      "\n",
      "Reading data from disk ...\n",
      "   Read properties file ...\n",
      "   ...\n",
      "   One Hot Encoding ...\n",
      "   ...\n",
      "   Read training file ...\n",
      "   ...\n",
      "((167888, 177), (167888L,))\n",
      "\n",
      "Prepare for prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "(2985217, 177)\n",
      "(167888L, 177L)\n"
     ]
    }
   ],
   "source": [
    "print( \"\\n\\nProcessing data for Neural Network ...\")\n",
    "\n",
    "x_train,y_train,x_test,parcelid=loader(prop_lightgbm_path,['parcelid', 'logerror','transactiondate']+extra_features,parse_date=True,onehotencode=True)\n",
    "ssr=normalize(x_test)\n",
    "x_train=ssr.transform(x_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "len_x=x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up neural network model...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSetting up neural network model...\")\n",
    "nn = Sequential()\n",
    "nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = len_x))\n",
    "nn.add(PReLU())\n",
    "nn.add(Dropout(.4))\n",
    "nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.5))\n",
    "nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(1, kernel_initializer='normal'))\n",
    "nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation For NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "Cross Validated Mean Absolute Error :     0.0670682084726\n"
     ]
    }
   ],
   "source": [
    "kf=KFold(n_splits=5)\n",
    "loss=[]\n",
    "for train_index, test_index in kf.split(x_train,y_train):\n",
    "    nncvhistory=nn.fit(np.array(x_train[train_index]), np.array(y_train[train_index]), batch_size = 4096, epochs = 35, verbose=0,validation_data=(np.array(x_train[test_index]), np.array(y_train[test_index])))\n",
    "    loss.append(nncvhistory.history['val_loss'][-1])\n",
    "    print('   ...')\n",
    "\n",
    "print('Cross Validated Mean Absolute Error :     %s'%(str(np.mean(loss))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting neural network model...\n",
      "0.0674957613229\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFitting neural network model...\")\n",
    "\n",
    "nnhistory=nn.fit(np.array(x_train), np.array(y_train), batch_size = 4096, epochs = 70, verbose=0)\n",
    "\n",
    "print(nnhistory.history['loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with neural network model...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "\n",
      "Neural Network predictions:\n",
      "   parcelid    201610    201611    201612    201710    201711    201712\n",
      "0  10754147  0.010996  0.015159  0.020081  0.015015  0.019944  0.025643\n",
      "1  10759547  0.040656  0.046502  0.053228  0.056032  0.062591  0.069539\n",
      "2  10843547  0.055088  0.061783  0.068511  0.068065  0.074808  0.081741\n",
      "3  10859147  0.050606  0.056030  0.061424  0.058380  0.063728  0.069067\n",
      "4  10879947  0.010137  0.015866  0.022031  0.022216  0.027066  0.032445\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredicting with neural network model...\")\n",
    "\n",
    "preds_dictionary=OrderedDict()\n",
    "preds_dictionary['parcelid']=parcelid\n",
    "for i,date in enumerate(prediction_columns):\n",
    "    x_test_i=x_test.copy()\n",
    "    x_test_i[\"transactiondate_year\"] = date[:4]\n",
    "    x_test_i[\"transactiondate_month\"] = date[4:]\n",
    "    x_test_i=ssr.transform(x_test_i)\n",
    "    preds_dictionary[date]=nn.predict(x_test_i).flatten()\n",
    "    print \"   ...\"\n",
    "    del x_test_i\n",
    "nn_pred = pd.DataFrame(preds_dictionary)\n",
    "\n",
    "print( \"\\nNeural Network predictions:\" )\n",
    "print( pd.DataFrame(nn_pred).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2858"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train,y_train,x_test,parcelid,preds_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. OLS\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data For OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing data for OLS ...\n",
      "\n",
      "Reading data from disk ...\n",
      "   Read properties file ...\n",
      "   ...\n",
      "   One Hot Encoding ...\n",
      "   ...\n",
      "   Read training file ...\n",
      "   ...\n",
      "((167888, 177), (167888L,))\n",
      "\n",
      "Prepare for prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "(2985217, 177)\n",
      "(167888L, 177L)\n"
     ]
    }
   ],
   "source": [
    "print( \"\\n\\nProcessing data for OLS ...\")\n",
    "x_train,y_train,x_test,parcelid=loader(prop_lightgbm_path,['parcelid', 'logerror','transactiondate']+extra_features,parse_date=True,onehotencode=True)\n",
    "ssr=normalize(x_test)\n",
    "x_train=ssr.transform(x_train)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation For OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross validing OLS model ...\n",
      "Cross Validated Mean Absolute Error :     0.0698411270332\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCross validing OLS model ...\")\n",
    "\n",
    "reg = LinearRegression(n_jobs=-1)\n",
    "score=cross_val_score(reg,x_train, y_train, scoring=make_scorer(mean_absolute_error), cv=5)\n",
    "\n",
    "print('Cross Validated Mean Absolute Error :     %s'%(str(np.mean(score))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting OLS...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nFitting OLS...\")\n",
    "\n",
    "reg = LinearRegression(n_jobs=-1)\n",
    "reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with OLS model...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "\n",
      "OLS predictions:\n",
      "   parcelid        201610        201611        201612        201710  \\\n",
      "0  10754147  5.206299e-02  5.243874e-02  5.281448e-02  5.705261e-02   \n",
      "1  10759547  6.622478e+11  6.622478e+11  6.622478e+11  6.622478e+11   \n",
      "2  10843547  9.053040e-02  9.090614e-02  9.128189e-02  9.552002e-02   \n",
      "3  10859147  5.348206e-02  5.385780e-02  5.423355e-02  5.847168e-02   \n",
      "4  10879947  6.269073e-02  6.306648e-02  6.344223e-02  6.768036e-02   \n",
      "\n",
      "         201711        201712  \n",
      "0  5.742836e-02  5.780411e-02  \n",
      "1  6.622478e+11  6.622478e+11  \n",
      "2  9.589577e-02  9.627151e-02  \n",
      "3  5.884743e-02  5.922318e-02  \n",
      "4  6.805611e-02  6.843185e-02  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredicting with OLS model...\")\n",
    "\n",
    "preds_dictionary=OrderedDict()\n",
    "preds_dictionary['parcelid']=parcelid\n",
    "for i,date in enumerate(prediction_columns):\n",
    "    x_test_i=x_test.copy()\n",
    "    x_test_i[\"transactiondate_year\"] = date[:4]\n",
    "    x_test_i[\"transactiondate_month\"] = date[4:]\n",
    "    x_test_i=ssr.transform(x_test_i)\n",
    "    preds_dictionary[date]=reg.predict(x_test_i)\n",
    "    print \"   ...\"\n",
    "    del x_test_i\n",
    "ols_pred = pd.DataFrame(preds_dictionary)\n",
    "\n",
    "print( \"\\nOLS predictions:\" )\n",
    "print( pd.DataFrame(ols_pred).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train,y_train,x_test,preds_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Combine And Save\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Predictionsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2985217L, 6L)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining XGBoost, LightGBM, and baseline predicitons ...\n",
      "\n",
      "Combined XGB/LGB/baseline predictions:\n",
      "          0\n",
      "0  0.017922\n",
      "1  0.003504\n",
      "2 -0.023398\n",
      "3  0.001861\n",
      "4 -0.022502\n",
      "\n",
      "Combining with XGB/LGB/NN/OLS/baseline predicitons: ...\n",
      "\n",
      "Combined XGB/LGB/NN/baseline/OLS predictions:\n",
      "   parcelid        201610        201611        201612        201710  \\\n",
      "0  10754147  2.340000e-02  2.380000e-02  2.430000e-02  2.410000e-02   \n",
      "1  10759547  4.598649e+10  4.598649e+10  4.598649e+10  4.598649e+10   \n",
      "2  10843547 -1.340000e-02 -1.270000e-02 -1.210000e-02 -1.180000e-02   \n",
      "3  10859147  1.020000e-02  1.070000e-02  1.120000e-02  1.120000e-02   \n",
      "4  10879947 -1.840000e-02 -1.780000e-02 -1.730000e-02 -1.690000e-02   \n",
      "\n",
      "         201711        201712  \n",
      "0  2.460000e-02  2.510000e-02  \n",
      "1  4.598649e+10  4.598649e+10  \n",
      "2 -1.120000e-02 -1.060000e-02  \n",
      "3  1.180000e-02  1.230000e-02  \n",
      "4 -1.650000e-02 -1.600000e-02  \n"
     ]
    }
   ],
   "source": [
    "print( \"\\nCombining XGBoost, LightGBM, and baseline predicitons ...\" )\n",
    "lgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - NN_WEIGHT - OLS_WEIGHT \n",
    "lgb_weight0 = lgb_weight / (1 - OLS_WEIGHT)\n",
    "xgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\n",
    "baseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\n",
    "nn_weight0 = NN_WEIGHT / (1 - OLS_WEIGHT)\n",
    "pred0 = baseline_weight0*BASELINE_PRED\n",
    "pred0 += lgb_weight0*lgb_pred\n",
    "pred0 += xgb_weight0*xgb_pred\n",
    "\n",
    "print( \"\\nCombined XGB/LGB/baseline predictions:\" )\n",
    "print( pd.DataFrame(pred0).head() )\n",
    "\n",
    "print( \"\\nCombining with XGB/LGB/NN/OLS/baseline predicitons: ...\" )\n",
    "pred0 =np.tile(pred0[:,None],[1,6]) + nn_weight0*nn_pred.values[:,1:]\n",
    "pred0 = FUDGE_FACTOR * ( OLS_WEIGHT*ols_pred.values[:,1:] + (1-OLS_WEIGHT)*pred0 )\n",
    "submission_dict=OrderedDict()\n",
    "submission_dict['parcelid']=parcelid\n",
    "for i,date in enumerate(prediction_columns):\n",
    "    submission_dict[date]=pred0[:,i]\n",
    "submission=pd.DataFrame(submission_dict)\n",
    "for c in submission.columns[1:]:\n",
    "    submission[c]=submission[c].apply(lambda x:float(format(x, '.4f')))\n",
    "\n",
    "print( \"\\nCombined XGB/LGB/NN/baseline/OLS predictions:\" )\n",
    "print( submission.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing results to disk ...\n",
      "\n",
      "Finished ...\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print( \"\\nWriting results to disk ...\" )\n",
    "submission.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n",
    "\n",
    "print( \"\\nFinished ...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Predicting\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forked From Kaggle - Andy Harless - XGBoost, LightGBM, OLS and NN\n",
    "Public Score\n",
    "0.0643646"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Global Hyper Parameters\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FUDGE_FACTOR = 1.1200  # Multiply forecasts by this\n",
    "\n",
    "XGB_WEIGHT = 0.6200\n",
    "BASELINE_WEIGHT = 0.0100\n",
    "OLS_WEIGHT = 0.0620\n",
    "NN_WEIGHT = 0.0800\n",
    "\n",
    "XGB1_WEIGHT = 0.8000  # Weight of first in combination of two XGB models\n",
    "\n",
    "BASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import Packages\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 5110 on context None\n",
      "Preallocating 9011/11264 Mb (0.800000) on cuda0\n",
      "Mapped name None to device cuda0: GeForce GTX 1080 Ti (0000:06:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler,Normalizer\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression,Ridge,RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error,make_scorer\n",
    "from sklearn.model_selection import cross_val_score,KFold,GridSearchCV\n",
    "import random\n",
    "import datetime as dt\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Preparation\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_only=True\n",
    "clip_norm=(-0.4,0.419)\n",
    "month_train=range(1,7)\n",
    "month_test=range(7,10)\n",
    "prediction_columns=['201607','201608','201609','201707','201708','201709']\n",
    "apath='E:/Data/zillow'\n",
    "spath='poly'\n",
    "prop_lightgbm_path=apath+'/sets/'+spath+'/tree.csv'\n",
    "prop_xgboost_path=apath+'/sets/'+spath+'/tree.csv'\n",
    "prop_ols_path=apath+'/sets/'+spath+'/linear.csv'\n",
    "prop_nn_path=apath+'/sets/'+spath+'/linear.csv'\n",
    "train_path=apath+'/train.csv'\n",
    "sample_path=apath+'/bak/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loader(properties_path,drop_list,outlier_bound=(None,None),parse_date=False):\n",
    "    print( \"\\nReading data from disk ...\")\n",
    "    print(\"   Read properties file ...\")\n",
    "    properties = pd.read_csv(properties_path)\n",
    "    for c, dtype in zip(properties.columns, properties.dtypes):\n",
    "        if dtype == np.float64:\n",
    "            properties[c] = properties[c].astype('float32')\n",
    "    print(\"   Read training file ...\")\n",
    "    train = pd.read_csv(train_path, parse_dates=[\"transactiondate\"])\n",
    "    train_properties = train.merge(properties, how='left', on='parcelid')\n",
    "    if parse_date:\n",
    "        train_properties[\"transactiondate_year\"] = train_properties[\"transactiondate\"].dt.year\n",
    "        train_properties[\"transactiondate_month\"] = train_properties[\"transactiondate\"].dt.month\n",
    "    if outlier_bound[0] is not None:\n",
    "        train_properties=train_properties[train_properties.logerror > outlier_bound[0]]\n",
    "    if outlier_bound[1] is not None:\n",
    "        train_properties=train_properties[train_properties.logerror < outlier_bound[1]]\n",
    "    \n",
    "    cv_train_properties=train_properties.copy()\n",
    "    cv_train_drop_list=copy.copy(drop_list)\n",
    "    cv_train_drop_list_all=copy.copy(cv_train_drop_list)\n",
    "    for dropitem in cv_train_drop_list_all:\n",
    "        if dropitem not in cv_train_properties.columns.tolist():\n",
    "            cv_train_drop_list.remove(dropitem)\n",
    "    y_cv_train = train_properties['logerror']\n",
    "    x_cv_train = train_properties.drop(cv_train_drop_list, axis=1)\n",
    "    print(x_cv_train.shape, y_cv_train.shape)\n",
    "    \n",
    "    train_properties=train_properties[train_properties['transactiondate'].dt.month.isin(month_train)]\n",
    "    train_drop_list=copy.copy(drop_list)\n",
    "    train_drop_list_all=copy.copy(train_drop_list)\n",
    "    for dropitem in train_drop_list_all:\n",
    "        if dropitem not in train_properties.columns.tolist():\n",
    "            train_drop_list.remove(dropitem)\n",
    "    y_train = train_properties['logerror']\n",
    "    x_train = train_properties.drop(train_drop_list, axis=1)\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    \n",
    "    print(\"\\nPrepare for prediction ...\")\n",
    "    print(\"   Read sample file ...\")\n",
    "    sample = pd.read_csv(sample_path)\n",
    "    sample['parcelid'] = sample['ParcelId']\n",
    "    print(\"   ...\")\n",
    "    print(\"   Merge with property data ...\")\n",
    "    if test_only:\n",
    "        test_properties = train.merge(properties, how='left', on='parcelid')\n",
    "        test_properties=test_properties[test_properties['transactiondate'].dt.month.isin(month_test)]\n",
    "    else:\n",
    "        test_properties = sample.merge(properties, on='parcelid', how='left')\n",
    "    parcelid=test_properties['parcelid']\n",
    "    if parse_date:\n",
    "        test_properties[\"transactiondate_year\"] = 2016.5\n",
    "        test_properties[\"transactiondate_month\"] = np.mean(month_test)\n",
    "    test_drop_list=copy.copy(drop_list)+['ParcelId','201610','201611','201612','201710','201711','201712']\n",
    "    test_drop_list_all=copy.copy(test_drop_list)\n",
    "    for dropitem in test_drop_list_all:\n",
    "        if dropitem not in test_properties.columns.tolist():\n",
    "            test_drop_list.remove(dropitem)\n",
    "    x_test=test_properties.drop(test_drop_list, axis=1)\n",
    "    print(x_test.shape)\n",
    "    return x_cv_train,y_cv_train,x_train,y_train,x_test,parcelid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x_test):\n",
    "    ssr=StandardScaler()\n",
    "    ssr.fit(x_test)\n",
    "    return ssr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_catgory(*all_sets):\n",
    "    for i,sets in enumerate(all_sets):\n",
    "        for c, dtype in zip(sets.columns, sets.dtypes):\n",
    "            if c=='parcelid':\n",
    "                pass\n",
    "            elif dtype == np.float32:\n",
    "                pass\n",
    "            elif dtype == np.float64:\n",
    "                all_sets[i][c] = sets[c].astype('float32')\n",
    "            elif dtype ==  np.bool:\n",
    "                all_sets[i][c] = sets[c].astype(\"category\")\n",
    "            elif dtype ==  np.int:\n",
    "                all_sets[i][c] = sets[c].astype(\"category\")\n",
    "            elif dtype ==  np.int8:\n",
    "                all_sets[i][c] = sets[c].astype(\"category\")\n",
    "            elif dtype ==  np.int16:\n",
    "                all_sets[i][c] = sets[c].astype(\"category\")\n",
    "            elif dtype ==  np.int32:\n",
    "                all_sets[i][c] = sets[c].astype(\"category\")\n",
    "            elif dtype ==  np.int64:\n",
    "                all_sets[i][c] = sets[c].astype(\"category\")\n",
    "    return all_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid = pd.read_csv(train_path, parse_dates=[\"transactiondate\"])\n",
    "y_mask=[]\n",
    "y_true = valid.logerror[valid['transactiondate'].dt.month.isin(month_test)]\n",
    "y_date = valid.transactiondate[valid['transactiondate'].dt.month.isin(month_test)]\n",
    "for yr in [2016,2017]:\n",
    "    for mt in month_test:\n",
    "        y_mask.append(np.bitwise_and(y_date.dt.month==mt,y_date.dt.year==yr))\n",
    "y_mask=np.asarray(y_mask)\n",
    "y_mask=y_mask.T.astype('int64')\n",
    "def validate(y_test):\n",
    "    return np.mean(np.abs(np.sum(y_test*y_mask,1)-y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06753257088954569"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.ones_like(y_mask).astype('float32')*BASELINE_PRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. LightGBM\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for LightGBM ...\n",
      "\n",
      "Reading data from disk ...\n",
      "   Read properties file ...\n",
      "   Read training file ...\n",
      "((167888, 148), (167888L,))\n",
      "((105121, 148), (105121L,))\n",
      "\n",
      "Prepare for prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "(54225, 148)\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nProcessing data for LightGBM ...\" )\n",
    "\n",
    "x_cv_train,y_cv_train,x_train,y_train,x_test,parcelid=loader(prop_lightgbm_path,['parcelid', 'logerror', 'transactiondate'])\n",
    "x_cv_train,x_train,x_test=make_catgory(x_cv_train,x_train,x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set LightGBM Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['max_bin'] = 10\n",
    "params['learning_rate'] = 0.0021 # shrinkage_rate\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'mae'          # or 'mae'\n",
    "params['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\n",
    "params['bagging_fraction'] = 0.85 # sub_row\n",
    "params['bagging_freq'] = 40\n",
    "params['num_leaves'] = 512        # num_leaf\n",
    "params['min_data'] = 500         # min_data_in_leaf\n",
    "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n",
    "params['verbose'] = 0\n",
    "params['feature_fraction_seed'] = 2\n",
    "params['bagging_seed'] = 3\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross validing LightGBM model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aeloyq\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validated Mean Absolute Error :     0.0621301388532\n",
      "Cross Validated Mean Absolute Error Std :     0.00886757245518\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCross validing LightGBM model ...\")\n",
    "\n",
    "d_cv_train = lgb.Dataset(x_cv_train, label=y_cv_train ,free_raw_data=False)\n",
    "score=lgb.cv(params, d_cv_train, 430, metrics='mae')\n",
    "\n",
    "print('Cross Validated Mean Absolute Error :     %s'%(str(score.values()[0][-1])))\n",
    "print('Cross Validated Mean Absolute Error Std :     %s'%(np.mean(score.values()[1][-1])))\n",
    "del d_cv_train,score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting LightGBM model ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFitting LightGBM model ...\")\n",
    "\n",
    "d_train = lgb.Dataset(x_train, label=y_train ,free_raw_data=False)\n",
    "clf = lgb.train(params, d_train, 430)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lightgbm_plot=lgb.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lightgbm_plot_digraph=lgb.create_tree_digraph(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_plot.figure.savefig('Results/lightgbm_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Results/lightgbm_plot_digraph.png'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightgbm_plot_digraph.save('Results/lightgbm_plot_digraph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start LightGBM prediction ...\n",
      "\n",
      "Unadjusted LightGBM predictions:\n",
      "          0\n",
      "0 -0.003978\n",
      "1  0.010381\n",
      "2  0.003378\n",
      "3  0.011961\n",
      "4  0.006336\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStart LightGBM prediction ...\")\n",
    "\n",
    "lgb_pred = clf.predict(x_test)\n",
    "\n",
    "print( \"\\nUnadjusted LightGBM predictions:\" )\n",
    "print( pd.DataFrame(lgb_pred).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate For LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06725869844025725"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(np.tile(lgb_pred[:,None],[1,6]),clip_norm[0],clip_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. XGBoost\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data For XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for XGBoost ...\n",
      "\n",
      "Reading data from disk ...\n",
      "   Read properties file ...\n",
      "   Read training file ...\n",
      "((164477, 148), (164477L,))\n",
      "((102914, 148), (102914L,))\n",
      "\n",
      "Prepare for prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "(54225, 148)\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nProcessing data for XGBoost ...\")\n",
    "\n",
    "x_cv_train,y_cv_train,x_train,y_train,x_test,parcelid=loader(prop_xgboost_path,['parcelid', 'logerror', 'transactiondate'],outlier_bound=clip_norm)\n",
    "y_mean = np.mean(y_train)\n",
    "dcvtrain = xgb.DMatrix(x_cv_train, y_cv_train)\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set XGBoost Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "num_boost_rounds = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross validing XGBoost model ...\n",
      "Cross Validated Mean Absolute Error :     0.0526324\n",
      "Cross Validated Mean Absolute Error Std :     0.000435497233057\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCross validing XGBoost model ...\")\n",
    "\n",
    "score=xgb.cv(dict(xgb_params, silent=1), dcvtrain, num_boost_round=num_boost_rounds, nfold=5, metrics='mae')\n",
    "\n",
    "print('Cross Validated Mean Absolute Error :     %s'%(str(score.iloc[-1,0])))\n",
    "print('Cross Validated Mean Absolute Error Std :     %s'%(str(score.iloc[-1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting XGBoost model ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFitting XGBoost model ...\")\n",
    "\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_1_plot=xgb.plot_tree(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_1_plot_graphviz=xgb.to_graphviz(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAACzCAYAAABvnKA2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b389ae10>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_1_plot.figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.pdf'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_1_plot_graphviz.save('1.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with XGBoost ...\n",
      "\n",
      "First XGBoost predictions:\n",
      "          0\n",
      "0 -0.013637\n",
      "1  0.003126\n",
      "2 -0.002773\n",
      "3  0.006777\n",
      "4 -0.001322\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nPredicting with XGBoost ...\")\n",
    "\n",
    "xgb_pred1 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nFirst XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred1).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate For First XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06697292111501733"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(np.tile(xgb_pred1[:,None],[1,6]),clip_norm[0],clip_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set XGBoost Hyper Parameters Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'eta': 0.033,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "num_boost_rounds = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross validing XGBoost model ...\n",
      "Cross Validated Mean Absolute Error :     0.0526452\n",
      "Cross Validated Mean Absolute Error Std :     0.000440325061744\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCross validing XGBoost model ...\")\n",
    "\n",
    "score=xgb.cv(dict(xgb_params, silent=1), dcvtrain, num_boost_round=num_boost_rounds, nfold=5, metrics='mae')\n",
    "\n",
    "print('Cross Validated Mean Absolute Error :     %s'%(str(score.iloc[-1,0])))\n",
    "print('Cross Validated Mean Absolute Error Std :     %s'%(str(score.iloc[-1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run XGBoost Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost again ...\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nTraining XGBoost again ...\")\n",
    "\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgboost_2_plot=xgb.plot_tree(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgboost_2_plot_graphviz=xgb.to_graphviz(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Prediction Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with XGBoost again ...\n",
      "\n",
      "Second XGBoost predictions:\n",
      "          0\n",
      "0 -0.004858\n",
      "1  0.003548\n",
      "2 -0.000146\n",
      "3  0.009720\n",
      "4  0.002261\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nPredicting with XGBoost again ...\")\n",
    "\n",
    "xgb_pred2 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nSecond XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred2).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate For Second XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06700912723092256"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(np.tile(xgb_pred2[:,None],[1,6]),clip_norm[0],clip_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Twice XGBoost Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined XGBoost predictions:\n",
      "          0\n",
      "0 -0.011881\n",
      "1  0.003211\n",
      "2 -0.002248\n",
      "3  0.007366\n",
      "4 -0.000605\n"
     ]
    }
   ],
   "source": [
    "xgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n",
    "\n",
    "print( \"\\nCombined XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate For Combined XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06696560300365594"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(np.tile(xgb_pred[:,None],[1,6]),clip_norm[0],clip_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1443"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_cv_train,y_cv_train,x_train,y_train,x_test,parcelid,dcvtrain,dtrain,dtest\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Neural Network\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data For Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing data for Neural Network ...\n",
      "\n",
      "Reading data from disk ...\n",
      "   Read properties file ...\n",
      "   Read training file ...\n",
      "((167888, 349), (167888L,))\n",
      "((105121, 349), (105121L,))\n",
      "\n",
      "Prepare for prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "(54225, 349)\n"
     ]
    }
   ],
   "source": [
    "print( \"\\n\\nProcessing data for Neural Network ...\")\n",
    "\n",
    "x_cv_train,y_cv_train,x_train,y_train,x_test,parcelid=loader(prop_nn_path,['parcelid', 'logerror','transactiondate'],parse_date=True)\n",
    "len_x=x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up neural network model...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSetting up neural network model...\")\n",
    "nn = Sequential()\n",
    "nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = len_x))\n",
    "nn.add(PReLU())\n",
    "nn.add(Dropout(.4))\n",
    "nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.5))\n",
    "nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(1, kernel_initializer='normal'))\n",
    "nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "plot_model(nn, to_file='nnmodel.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "Cross Validated Mean Absolute Error :     0.068905257085\n"
     ]
    }
   ],
   "source": [
    "kf=KFold(n_splits=5)\n",
    "loss=[]\n",
    "for train_index, test_index in kf.split(x_cv_train,y_cv_train):\n",
    "    nncvhistory=nn.fit(x_cv_train.values[train_index], y_cv_train.values[train_index], batch_size = 4096, epochs = 35, verbose=0,validation_data=(x_cv_train.values[test_index], y_cv_train.values[test_index]))\n",
    "    loss.append(nncvhistory.history['val_loss'][-1])\n",
    "    print('   ...')\n",
    "\n",
    "print('Cross Validated Mean Absolute Error :     %s'%(str(np.mean(loss))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting neural network model...\n",
      "0.0698079611911\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFitting neural network model...\")\n",
    "\n",
    "nnhistory=nn.fit(np.array(x_train), np.array(y_train), batch_size = 4096, epochs = 70, verbose=0)\n",
    "\n",
    "print(nnhistory.history['loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with neural network model...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "\n",
      "Neural Network predictions:\n",
      "      parcelid    201607    201608    201609    201707    201708    201709\n",
      "782   14677191  0.006206  0.006206  0.006206  0.006206  0.006206  0.006206\n",
      "968   11183209  0.006207  0.006207  0.006207  0.006207  0.006207  0.006207\n",
      "1165  11554091  0.006207  0.006207  0.006207  0.006207  0.006207  0.006207\n",
      "1351  11742566  0.006208  0.006208  0.006208  0.006208  0.006208  0.006208\n",
      "1609  14667297  0.006206  0.006206  0.006206  0.006206  0.006206  0.006206\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredicting with neural network model...\")\n",
    "\n",
    "preds_dictionary=OrderedDict()\n",
    "preds_dictionary['parcelid']=parcelid\n",
    "for i,date in enumerate(prediction_columns):\n",
    "    x_test_i=x_test.copy()\n",
    "    x_test_i[\"transactiondate_year\"] = date[:4]\n",
    "    x_test_i[\"transactiondate_month\"] = date[4:]\n",
    "    preds_dictionary[date]=nn.predict(x_test_i.values).flatten()\n",
    "    print \"   ...\"\n",
    "    del x_test_i\n",
    "nn_pred = pd.DataFrame(preds_dictionary)\n",
    "\n",
    "print( \"\\nNeural Network predictions:\" )\n",
    "print( pd.DataFrame(nn_pred).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate For NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06746285278703487"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(nn_pred.values[:,1:],clip_norm[0],clip_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4488"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del preds_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. OLS\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "random.seed(17)\n",
    "gsridge=GridSearchCV(Ridge(),{'alpha':np.e**np.arange(-5,5,0.2)},n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross validing OLS model ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "121.51041751873579"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nCross validing OLS model ...\")\n",
    "\n",
    "gsridge.fit(x_cv_train, y_cv_train)\n",
    "gsridge.best_params_['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross validing OLS model ...\n",
      "Cross Validated Mean Absolute Error :     0.0700925928284\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCross validing OLS model ...\")\n",
    "\n",
    "reg = Ridge(gsridge.best_params_['alpha'])\n",
    "score=cross_val_score(reg,x_cv_train, y_cv_train, scoring=make_scorer(mean_absolute_error), cv=5)\n",
    "\n",
    "print('Cross Validated Mean Absolute Error :     %s'%(str(np.mean(score))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting OLS...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=121.51041751873579, copy_X=True, fit_intercept=True,\n",
       "   max_iter=None, normalize=False, random_state=None, solver='auto',\n",
       "   tol=0.001)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nFitting OLS...\")\n",
    "\n",
    "reg = Ridge(gsridge.best_params_['alpha'])\n",
    "reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with OLS model...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "\n",
      "OLS predictions:\n",
      "      parcelid    201607    201608    201609    201707    201708    201709\n",
      "782   14677191 -0.020856 -0.022752 -0.024649 -0.016808 -0.018704 -0.020601\n",
      "968   11183209  0.004274  0.002377  0.000481  0.008322  0.006425  0.004529\n",
      "1165  11554091 -0.004914 -0.006810 -0.008707 -0.000866 -0.002762 -0.004659\n",
      "1351  11742566  0.023327  0.021431  0.019534  0.027375  0.025479  0.023582\n",
      "1609  14667297 -0.010746 -0.012643 -0.014539 -0.006698 -0.008595 -0.010491\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredicting with OLS model...\")\n",
    "\n",
    "preds_dictionary=OrderedDict()\n",
    "preds_dictionary['parcelid']=parcelid\n",
    "for i,date in enumerate(prediction_columns):\n",
    "    x_test_i=x_test.copy()\n",
    "    x_test_i[\"transactiondate_year\"] = date[:4]\n",
    "    x_test_i[\"transactiondate_month\"] = date[4:]\n",
    "    preds_dictionary[date]=reg.predict(x_test_i.values)\n",
    "    print \"   ...\"\n",
    "    del x_test_i\n",
    "ols_pred = pd.DataFrame(preds_dictionary)\n",
    "\n",
    "print( \"\\nOLS predictions:\" )\n",
    "print( pd.DataFrame(ols_pred).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate For OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06822719904974628"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(ols_pred.values[:,1:],clip_norm[0],clip_norm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train,y_train,x_test,preds_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Combine And Save\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Predictionsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining XGBoost, LightGBM, and baseline predicitons ...\n",
      "\n",
      "Combined XGB/LGB/baseline predictions:\n",
      "          0\n",
      "0 -0.008158\n",
      "1  0.004473\n",
      "2 -0.000508\n",
      "3  0.007409\n",
      "4  0.001184\n",
      "\n",
      "Combining with XGB/LGB/NN/OLS/baseline predicitons: ...\n",
      "\n",
      "Combined XGB/LGB/NN/baseline/OLS predictions:\n",
      "      parcelid  201607  201608  201609  201707  201708  201709\n",
      "782   14677191 -0.0100 -0.0102 -0.0103 -0.0097 -0.0099 -0.0100\n",
      "968   11183209  0.0059  0.0057  0.0056  0.0061  0.0060  0.0059\n",
      "1165  11554091 -0.0004 -0.0005 -0.0006 -0.0001 -0.0002 -0.0003\n",
      "1351  11742566  0.0105  0.0103  0.0102  0.0108  0.0106  0.0105\n",
      "1609  14667297  0.0011  0.0010  0.0009  0.0014  0.0013  0.0012\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nCombining XGBoost, LightGBM, and baseline predicitons ...\" )\n",
    "lgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - NN_WEIGHT - OLS_WEIGHT \n",
    "pred0 = BASELINE_WEIGHT*BASELINE_PRED\n",
    "pred0 += lgb_weight*lgb_pred\n",
    "pred0 += XGB_WEIGHT*xgb_pred\n",
    "\n",
    "print( \"\\nCombined XGB/LGB/baseline predictions:\" )\n",
    "print( pd.DataFrame(pred0).head() )\n",
    "\n",
    "print( \"\\nCombining with XGB/LGB/NN/OLS/baseline predicitons: ...\" )\n",
    "pred0 =np.tile(pred0[:,None],[1,6]) + NN_WEIGHT*nn_pred.values[:,1:]\n",
    "pred0 = FUDGE_FACTOR * ( OLS_WEIGHT*ols_pred.values[:,1:] + pred0 )\n",
    "submission_dict=OrderedDict()\n",
    "submission_dict['parcelid']=parcelid\n",
    "for i,date in enumerate(prediction_columns):\n",
    "    submission_dict[date]=pred0[:,i]\n",
    "submission=pd.DataFrame(submission_dict)\n",
    "for c in submission.columns[1:]:\n",
    "    submission[c]=submission[c].apply(lambda x:float(format(x, '.4f')))\n",
    "\n",
    "print( \"\\nCombined XGB/LGB/NN/baseline/OLS predictions:\" )\n",
    "print( submission.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06693147954517052"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(np.clip(submission.values[:,1:],clip_norm[0],clip_norm[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
